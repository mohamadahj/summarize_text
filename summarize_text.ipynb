{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6357b432-ffee-4334-87b5-bbed0b64340f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸŸ¤ Bronze Layer: Raw Data Ingestion\n",
    "\n",
    "This notebook step performs the initial ingestion of raw text data from the provided CSV file into a Spark DataFrame (`raw_df`).  \n",
    "\n",
    "**Data Source:**\n",
    "- **File**: `AI_Data_Developer_Technical_Challenge___Sample_Raw_Data.csv`\n",
    "- **Format**: CSV\n",
    "- **Header Included**: Yes\n",
    "- **Schema Inference**: Disabled (all columns initially read as strings)\n",
    "\n",
    "The output displays the first 10 rows of the raw dataset to confirm successful data ingestion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8dc3b49-1f93-4742-aea3-4af83de35c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>category</th><th>text</th></tr></thead><tbody><tr><td>1</td><td>customer_review</td><td>I recently filed a claim with Rival Insurance, and the process was slower than expected. The customer service was helpful, but I had to follow up multiple times.</td></tr><tr><td>2</td><td>customer_review</td><td>Amazing experience! Rival Insurance approved my claim in just two days. Very satisfied with the service.</td></tr><tr><td>3</td><td>customer_review</td><td>Frustrating experience. My claim was denied without a clear explanation. I will consider switching providers.</td></tr><tr><td>4</td><td>policy_document</td><td>Policyholder agrees to maintain comprehensive coverage for damages not caused by collision, including theft and natural disasters.</td></tr><tr><td>5</td><td>policy_document</td><td>This insurance policy covers personal injury protection (PIP) up to $50,000 per insured person per accident.</td></tr><tr><td>6</td><td>policy_document</td><td>Exclusions: This policy does not cover damages resulting from intentional acts, fraud, or unauthorized vehicle usage.</td></tr><tr><td>7</td><td>claim_note</td><td>Claim #11234: Reviewed documents and found discrepancies in reported damages. Requested additional evidence from claimant.</td></tr><tr><td>8</td><td>claim_note</td><td>Claim #23567: Approved for payout after verification of supporting documents and repair estimates.</td></tr><tr><td>9</td><td>claim_note</td><td>Claim #34890: Denied due to policy exclusion on pre-existing damages. Notified policyholder of appeal options.</td></tr><tr><td>10</td><td>customer_review</td><td>Had to wait on hold for over an hour before speaking to a representative. Not a great experience.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "customer_review",
         "I recently filed a claim with Rival Insurance, and the process was slower than expected. The customer service was helpful, but I had to follow up multiple times."
        ],
        [
         "2",
         "customer_review",
         "Amazing experience! Rival Insurance approved my claim in just two days. Very satisfied with the service."
        ],
        [
         "3",
         "customer_review",
         "Frustrating experience. My claim was denied without a clear explanation. I will consider switching providers."
        ],
        [
         "4",
         "policy_document",
         "Policyholder agrees to maintain comprehensive coverage for damages not caused by collision, including theft and natural disasters."
        ],
        [
         "5",
         "policy_document",
         "This insurance policy covers personal injury protection (PIP) up to $50,000 per insured person per accident."
        ],
        [
         "6",
         "policy_document",
         "Exclusions: This policy does not cover damages resulting from intentional acts, fraud, or unauthorized vehicle usage."
        ],
        [
         "7",
         "claim_note",
         "Claim #11234: Reviewed documents and found discrepancies in reported damages. Requested additional evidence from claimant."
        ],
        [
         "8",
         "claim_note",
         "Claim #23567: Approved for payout after verification of supporting documents and repair estimates."
        ],
        [
         "9",
         "claim_note",
         "Claim #34890: Denied due to policy exclusion on pre-existing damages. Notified policyholder of appeal options."
        ],
        [
         "10",
         "customer_review",
         "Had to wait on hold for over an hour before speaking to a representative. Not a great experience."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/AI_Data_Developer_Technical_Challenge___Sample_Raw_Data.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "raw_df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(raw_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f591d402-6d40-44ac-9bdb-6805b0fab2a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/mnt/data/bronze\")\n",
    "dbutils.fs.mkdirs(\"/mnt/data/silver\")\n",
    "dbutils.fs.mkdirs(\"/mnt/data/gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041a39e2-a81b-4b11-b0d0-334e957094c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(\"/mnt/data/bronze/BronzeTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c66a4cd-4d1f-495d-8a6a-8775368be43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ¥ˆ Silver Layer: Data Cleaning and Preprocessing\n",
    "\n",
    "This step transforms the raw data (`bronze_df`) into a clean and structured format suitable for analytics and modeling.\n",
    "\n",
    "**Data Cleaning Steps Applied:**\n",
    "- **Duplicate Removal**: Eliminated duplicate records based on the original `text` column.\n",
    "- **Text Normalization**:\n",
    "  - Converted all text to lowercase.\n",
    "  - Removed special characters, retaining only alphanumeric and whitespace characters.\n",
    "  - Condensed multiple spaces into single spaces.\n",
    "  - Trimmed leading and trailing whitespace.\n",
    "- **Null and Empty Handling**: Removed records with empty or null text after cleaning.\n",
    "\n",
    "The resulting DataFrame (`cleaned_df`) represents high-quality, cleaned data ready for advanced processing in the Gold layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6644f27-ee49-4197-bd09-a48d4064041c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = spark.read.format(\"delta\").load(\"/mnt/data/bronze/BronzeTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89be3bf-d395-405f-87f9-6838672b36f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, trim, when\n",
    "\n",
    "# Clean Bronze data and prepare Silver-layer DataFrame\n",
    "cleaned_df = (bronze_df\n",
    "    # Remove duplicate records based on 'text' column\n",
    "    .dropDuplicates([\"text\"])\n",
    "\n",
    "    # Convert text to lowercase for consistency\n",
    "    .withColumn(\"clean_text\", lower(col(\"text\")))\n",
    "\n",
    "    # Remove all special characters (retain alphanumeric and whitespace)\n",
    "    .withColumn(\"clean_text\", regexp_replace(col(\"clean_text\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "    # Replace multiple consecutive whitespaces with a single space\n",
    "    .withColumn(\"clean_text\", regexp_replace(col(\"clean_text\"), \"\\\\s+\", \" \"))\n",
    "\n",
    "    # Remove leading/trailing whitespace\n",
    "    .withColumn(\"clean_text\", trim(col(\"clean_text\")))\n",
    "\n",
    "    # Set empty strings to null to filter them out later\n",
    "    .withColumn(\"clean_text\", when(col(\"clean_text\") == \"\", None).otherwise(col(\"clean_text\")))\n",
    "    \n",
    "    # Convert 'id' column explicitly to integer\n",
    "    .withColumn(\"id\", col(\"id\").cast(\"integer\"))                     \n",
    "\n",
    "    # Remove rows where 'clean_text' is null or empty after cleaning\n",
    "    .na.drop(subset=[\"clean_text\"])\n",
    ")\n",
    "\n",
    "\n",
    "# Write to Silver\n",
    "cleaned_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/mnt/data/silver/SilverCleanData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e439d9eb-7d72-41bc-a9a0-5a1a94eb9ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = spark.read.format(\"delta\").load(\"/mnt/data/silver/SilverCleanData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a693cf5-8c9d-4e7d-aa3b-cf4fdaa56558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ¥‡ Gold Layer: AI-Powered Text Summarization\n",
    "In this final stage, the cleaned data from the Silver layer is processed by the Azure OpenAI (GPT-4) Large Language Model (LLM) to produce concise summaries. This summarized dataset is intended for advanced analytics, reporting, or downstream consumption.\n",
    "\n",
    "Key Steps:\n",
    "\n",
    "- Summarize text using Azure OpenAI's GPT-4 model.\n",
    "- Implement robust error handling with retry logic and exponential backoff.\n",
    "- Leverage efficient batch processing (mapInPandas) for scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7156134-b2da-48c7-ac24-d6c68d7cc94c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "\n",
    "# Azure OpenAI Configuration \n",
    "endpoint = \"https://mahma-m851s8eb-eastus2.cognitiveservices.azure.com/\"\n",
    "model_name = \"gpt-4\"\n",
    "deployment = \"gpt-4\"\n",
    "\n",
    "subscription_key = dbutils.secrets.get(scope = \"azure-openai\", key = \"api_key\")\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "\n",
    "# Initialize client\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "# Summarization function with robust exception handling\n",
    "def summarize_text(text, client, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Summarize the following text concisely.\"},\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ],\n",
    "                max_tokens=100,\n",
    "                temperature=0.5,\n",
    "                top_p=0.9,\n",
    "                model=deployment\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(2 ** attempt)  # exponential backoff for API retry\n",
    "    return \"Summarization Failed\"\n",
    "\n",
    "# Batch summarization function for efficient Spark execution\n",
    "def batch_summarize(iterator) -> pd.DataFrame:\n",
    "    # Initialize Azure OpenAI client within each Spark partition (avoids serialization issues)\n",
    "    client = AzureOpenAI(\n",
    "        api_version=api_version,\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=subscription_key,\n",
    "    )\n",
    "\n",
    "    for pdf in iterator:\n",
    "         # Apply summarization on the 'clean_text' column\n",
    "        pdf['summary'] = pdf['clean_text'].apply(lambda x: summarize_text(x, client))\n",
    "        # Ensure 'id' column data type compatibility with Spark\n",
    "        pdf['id'] = pdf['id'].astype('int64')\n",
    "        # Return the DataFrame with required column\n",
    "        yield pdf[['id', 'text', 'clean_text', 'summary']]\n",
    "\n",
    "# Apply batch summarization with Spark's mapInPandas for scalability\n",
    "small_spark_df = silver_df.limit(3)\n",
    "gold_df = small_spark_df.mapInPandas(batch_summarize, schema=\"id long, text string, clean_text string, summary string\")\n",
    "\n",
    "\n",
    "# Persist the summarized data in Delta or Azure Blob Storage (Gold Layer)\n",
    "#gold_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/data/gold/gold_summarized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fec23e3-c8f0-4048-82d6-eddcb9cae85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“¦ Saving Summarized Data to Azure Blob Storage\n",
    "\n",
    "In this step, the summarized data (`gold_df`) from the Gold Layer is persisted to **Azure Blob Storage**. The data is stored in Parquet format, ensuring efficient retrieval and optimal performance for downstream analytics or API consumption.\n",
    "\n",
    "**Key Details:**\n",
    "- **Storage Account**: `textsummarizationstorage`\n",
    "- **Container Name**: `gold-layer`\n",
    "- **Output Path**: `gold-summarized/`\n",
    "- **Security**: Credentials (SAS tokens) securely managed via Databricks secrets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb473a1-1137-44a1-be7f-d8cd64e6a801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Azure storage account details\n",
    "storage_account = \"textsummarizationstorage\"\n",
    "container_name = \"gold-layer\"\n",
    "\n",
    "# Set Azure Blob Storage SAS token securely from Databricks Secrets\n",
    "sas_token = dbutils.secrets.get(scope = \"azure-storage\", key = \"sas_token\")\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{container_name}.{storage_account}.blob.core.windows.net\",\n",
    "    sas_token\n",
    ")\n",
    "\n",
    "# Define the Azure Blob Storage output path\n",
    "output_path = f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net/gold-summarized/\"\n",
    "\n",
    "# Write the summarized DataFrame to Blob Storage in Parquet format\n",
    "gold_df.write.mode(\"overwrite\").parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a67adf2-70f4-4c78-94e4-7fea23cbaa7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Retrieving Summarized Data from Azure Function API\n",
    "This step demonstrates how to fetch summarized text data from an Azure Function API endpoint directly into Databricks. This method allows our Databricks notebook to seamlessly integrate with external APIs, ensuring easy retrieval and further processing of our Gold-layer summarized data.\n",
    "\n",
    "Steps involved:\n",
    "\n",
    " - Securely fetching the Azure Function key from Databricks Secrets.\n",
    " - Making an HTTP GET request to the Azure Function API.\n",
    " - Handling and verifying the API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829668ef-1d4a-4a79-bf92-081aed7fd293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API call successful!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Azure Function endpoint and function key\n",
    "function_url = \"https://textsummarizationapi.azurewebsites.net/api/summaries\"\n",
    "function_key = dbutils.secrets.get(scope = \"azure-function-apps\", key = \"summarize_api_key\")\n",
    "\n",
    "# Make an HTTP GET request to fetch summarized data\n",
    "response = requests.get(f\"{function_url}?code={function_key}\")\n",
    "\n",
    "# Check response\n",
    "if response.status_code == 200:\n",
    "    # Successful API call, parse JSON response\n",
    "    data = response.json()\n",
    "    print(\"API call successful!\")\n",
    "else:\n",
    "    # API call failed; print status code and error details\n",
    "    print(f\"API call failed with status code: {response.status_code}\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ef1168-3297-410e-b51d-957528495465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>text</th><th>summary</th></tr></thead><tbody><tr><td>Claim #38945: Initial payout approved. Funds to be disbursed soon.</td><td>Claim #38945 has been approved for initial payout, with funds expected to be disbursed shortly.</td></tr><tr><td>Filed a claim through the mobile app. Process was smooth and easy.</td><td>The claim filing process via the mobile app was smooth and easy.</td></tr><tr><td>Claim #43298: Claim rejected due to policy exclusions.</td><td>Claim #43298 was rejected because it falls under policy exclusions.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Claim #38945: Initial payout approved. Funds to be disbursed soon.",
         "Claim #38945 has been approved for initial payout, with funds expected to be disbursed shortly."
        ],
        [
         "Filed a claim through the mobile app. Process was smooth and easy.",
         "The claim filing process via the mobile app was smooth and easy."
        ],
        [
         "Claim #43298: Claim rejected due to policy exclusions.",
         "Claim #43298 was rejected because it falls under policy exclusions."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load JSON response into Pandas DataFrame first\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(spark_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3037502006994147,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "summarize_text",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}